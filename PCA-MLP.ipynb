{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "Uma das formas mais tradicionais de redes neurais é a MLP, ou Multilayer Perceptron. \n",
    "A MLP é um algoritmo de aprendizado supervisionado que aprende uma função que relaciona entradas e saídas através do seu treinamento em um conjunto de dados.\n",
    "\n",
    "Tipicamente, uma MLP é formada por:\n",
    "- uma camada de entrada, que corresponde aos M atributos a serem usados para a classificação\n",
    "- uma camada de saída, que possui um neurônio para cada uma das N classes em que a amostra de atributos será classificada\n",
    "- uma ou mais camadas ocultas de neurônios, responsáveis por tornar a MLP um classificador não linear\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A instanciação de uma MLP\n",
    "\n",
    "Para criarmos uma MLP no `scikit-learn` usamos a classe `MLPClassifier` do pacote `sklearn.neural_network` através do código:\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "```\n",
    "A instanciação de um objeto representando a MLP depende da definição de **hiperparâmetros**, ou seja parâmetros da rede que não serão aprendidos durante a fase de treinamento. Até agora, os hiperparâmetros que aprendemos são:\n",
    "\n",
    "### 1. Número de camadas ocultas\n",
    "Indica quandas camadas ocultas serão usadas na rede neural. Há provas matemáticas de que uma rede do tipo MLP com função de ativação sigmóide pode aproximar qualquer função contínua com apenas uma única camada oculta de neurônios, porém para funções descontínuas, duas são necessárias (ver livro do Russel, pág. 720). Na prática, para dados simples, não precisamos de mais do que uma camada, e partimos para a decisão do número de neurônios nessa camada. O inconveniente dessa abordagem é que o número de neurônios necessários para realizar a classificação corretamente pode ter que crescer muito dependendo da complexidade dos dados. \n",
    "\n",
    "### 2. Número de neurônios por camada oculta\n",
    "No caso de escolhermos usar apenas uma camada oculta, é preciso escolher o número de neurônios a ser empregado.\n",
    "Há algumas \"receitas de bolo\" para essa decisão, porém a melhor forma de determinar esse parâmetro é realizar diversos testes com os dados de treinamento, através da *validação cruzada*. Uma dessas receitas de bolo indica que o número de neurônios da camada oculta seja o dobro do número de elementos de entrada mais um, que nós podemos usar como ponto de partida para a realização de testes.\n",
    "\n",
    "### 3. Função de ativação $f(a)$\n",
    "A função de ativação processa a entrada líquida do neurônio, resultante do somatório das entradas multiplicadas pelos pesos.\n",
    "Para a tarefa de classificação, as funções de ativação mais comumente empregadas são:\n",
    "- Logística ou sigmóide: $f(a)=\\frac{1}{1+e^{-a}}$\n",
    "<img width=\"400px\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1920px-Logistic-curve.svg.png\"/>\n",
    "- Tangente hiperbólica: $f(a) = tanh(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}$\n",
    "<img width=\"400px\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/250px-Hyperbolic_Tangent.svg.png\">\n",
    "\n",
    "- Linear retificada (ReLU): $f(a) = \\begin{cases} a, & \\mbox{se } a>=0 \\\\ 0, & \\mbox{caso contrário } \\end{cases}$\n",
    "<img width=\"400px\" src=\"http://ml4a.github.io/images/figures/relu.png\">\n",
    "\n",
    "\n",
    "Há também parâmetros relacionados ao processo de treinamento:\n",
    "- Taxa de aprendizado $\\eta$: valor maior do que zero que representa a velocidade em que ocorre o aprendizado\n",
    "- *Batch size*: número de amostras a serem usadas a cada atualização de pesos\n",
    "- Algoritmo de otimização: embora o algoritmo Backpropagation e suas variações seja o mais empregado, há outros algoritmos disponíveis para o trienamento. Aqui vamos trabalhar com o Backpropagation tradicional.\n",
    "- Número de épocas: determina quantas épocas, ou seja, quantas vezes cada amostra de entrada será utilizada, mesmo que o algoritmo de otimização não tenha encontrado um resultado confiável\n",
    "\n",
    "Dessa forma, a instanciação da nossa rede MLP fica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# Definição dos hiperparâmetros\n",
    "# Função de ativação: pode ser uma dentre: {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’\n",
    "activation = 'logistic' \n",
    "# Tamanhos das camadas ocultas\n",
    "hidden_layer_sizes = [100] # Apenas uma camada com cem neurônios\n",
    "# Algoritmo de otimização: pode ser um dentre {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n",
    "solver = 'sgd' #Stochastic Gradient Descent\n",
    "# Valor da taxa de aprendizado, default 0.001\n",
    "learning_rate =  0.03\n",
    "# Batch size, default é 'auto', ou seja, min(200, n_samples)\n",
    "batch_size=20\n",
    "# Número máximo de épocas, default 200\n",
    "max_iter = 1000\n",
    "mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                    activation=activation, solver=solver,\n",
    "                    learning_rate_init=learning_rate,\n",
    "                    batch_size=batch_size,\n",
    "                    max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificando Imagens com a MLP\n",
    "\n",
    "Na classificação de padrões de imagens, nosso objetivo é classificar o conteúdo de imagens digitais. Por exemplo, podemos segmentar os caracteres na imagem de um documento, e, para cada imagem de caractere, identificar qual é a letra ou dígito correspondente. Outro exemplo é identificar qual animal está presente na imagem (*cachorro*, *gato* ou *papagaio*, por exemplo).\n",
    "\n",
    "\n",
    "## 1. Gerando o vetor de atributos\n",
    "\n",
    "Para classificar o conteúdo de imagens digitais, uma abordagem muito comum é o uso dos seus próprios pixels como atributos. Assim, se uma imagem tem tamanho 100x100 pixels, terermos um vetor de atributos de 10.000 posições!\n",
    "- Vantagem: ganhamos flexibilidade no processo de classificação, já que não precisamos nos preocupar com a especificação de atributos específicos\n",
    "- Desvantagem: grande número de atributos, torna o treinamento muito lento e pode inclusive não finalizar em tempo hábil \n",
    "\n",
    "Assim vamos definir a função `gera_vetor(w, h, arq)` que toma um arquivo de imagem, faz um preprocessamento e gera o vetor de atributos. Uma vez que o vetor de atributos deve semrpe ter o mesmo tamanho, a imagem é reescalonada para `h` linhas (o que corresponde à altura, ou * **h**eight*) e `w` colunas (o que corresponde à sua largura, ou * **w**idth*) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def gera_vetor(w, h, arq):\n",
    "    \"\"\"\n",
    "    w: largura desejada para a imagem\n",
    "    h: altura desejada para a imagem\n",
    "    arq: Caminho do arquivo de imagem\n",
    "    retorno: array com o vetor de atributos, de tamanho w*h\n",
    "    \"\"\"\n",
    "    # Ler imagem como tons de cinza\n",
    "    img = cv2.imread(arq, cv2.IMREAD_GRAYSCALE)\n",
    "    # Pré-processamento: \n",
    "    # 1. Passa o filtro da mediana\n",
    "    # 2. gera imagem da magnitude do gradiente\n",
    "    # 3. Faz a média ponderada da imagem com o resutado da magnitude do gradiente\n",
    "    img = cv2.medianBlur(img, 3)\n",
    "    # O tipo de imagem CV_16 comporta pixels de 16bits com valores positivos e negativos\n",
    "    sobelx = cv2.Sobel(img, cv2.CV_16S, 1, 0)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_16S, 0, 1)\n",
    "    # Retorna ao tipo original de 8 bits unsigned (0 a 255)\n",
    "    mag_grad = cv2.add(np.abs(sobelx), np.abs(sobely), dtype=cv2.CV_8U)\n",
    "    img = cv2.addWeighted(img, 0.7, mag_grad, 0.3, gamma=0)\n",
    "    \n",
    "    #Redimensionando a imagem\n",
    "    img = cv2.resize(img, (w, h))\n",
    "    \n",
    "    # Transforma a imagem img em um array unidimensional e retorna\n",
    "    return img.ravel()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criando a matriz de dados de treinamento\n",
    "\n",
    "Para usar os algoritmos de classificação de padrões é necessário transformar as amostras de vetor de atributos em linhas da matriz de treinamento.\n",
    "\n",
    "Vamos assumir que as imagens representativas das diferentes classes estão armazenadas em diretórios homônimos. Assim, para cada imagem em cada diretório, iremos criar uma linha da matriz e uma entrada no array de classes, que chamaremos de `alvos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 784) (70,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "classes = [\"cachorro\", \"gato\", \"papagaio\"]\n",
    "\n",
    "# São os valores de altura e largura originais das imagens MNIST\n",
    "altura = 28\n",
    "largura = 28\n",
    "treinamento = []\n",
    "alvos = []\n",
    "for cls in classes:\n",
    "    for file in os.listdir(cls):\n",
    "        treinamento.append( gera_vetor(largura, altura, \"{}/{}\".format(cls,file)) )\n",
    "        alvos.append(cls)\n",
    "\n",
    "#Transforma em array\n",
    "treinamento = np.array(treinamento)\n",
    "alvos = np.array(alvos)\n",
    "\n",
    "print(treinamento.shape, alvos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reduzindo a dimensionalidade dos dados\n",
    "\n",
    "\n",
    "Um grande número de atributos pode atrapalhar o treinamento de classificadores, em especial a MLP.\n",
    "Para melhorar o desempenho do treinamento, vamos trabalhar os dados através da sua normalização e da redução da dimensionalidade.\n",
    "\n",
    "Reduzir a dimensionalidade significa diminuir as dimensões do vetor de atributos tentando manter a qualidade da informação ali contida. Uma das formas de fazer isso é através do PCA - *Principal Component Analysis* ou Análise de Componentes Principais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novo número de atributos:  26\n"
     ]
    }
   ],
   "source": [
    "# Normalizando os dados de treinamento\n",
    "media = treinamento.mean(axis=0)\n",
    "desvio = treinamento.std(axis=0)\n",
    "treinamento_norm = (treinamento - media)/desvio\n",
    "\n",
    "#Reduzindo a dimensionalidade\n",
    "from sklearn.decomposition import PCA\n",
    "# Instancia o modelo PCA de forma que 85% da variabilidade de dados seja mantida\n",
    "pca = PCA(.85)\n",
    "# Calcula o modelo e transforma os dados de treinamento\n",
    "treinamento_pca = pca.fit_transform(treinamento_norm)\n",
    "# Verificando o novo número de atributos\n",
    "print(\"Novo número de atributos: \", treinamento_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Avaliando o resultado da classificação \n",
    "\n",
    "Para podermos validar o resultado da classificação, temos que separar algumas amostras de treinamento para que sejam usadas para teste, sem utilizá-las no treinamento da rede neural. Assim, podemos comparar o resultado desse teste com as classes reais esperadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Separando as amostras de treinamento e validação\n",
    "frac_validacao = 0.2 # Separamos 20% para a validação\n",
    "# Primeiro embaralhamos os indices da matriz\n",
    "import numpy.random as random\n",
    "indices = random.permutation(treinamento_pca.shape[0])\n",
    "\n",
    "# ... depois separamos em porção de treinamento e validação\n",
    "import math\n",
    "icut = math.floor(frac_validacao * indices.shape[0])\n",
    "indices_treino = indices[icut:]\n",
    "indices_valid = indices[:icut]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Treinando e executando a rede neural criada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size=20, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=[100], learning_rate='constant',\n",
       "       learning_rate_init=0.03, max_iter=1000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Treinar o classificador MLP instanciado anteriormente, seprando as entradas e saídas \n",
    "mlp.fit(treinamento_pca[indices_treino,:], alvos[indices_treino])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma observação sobre a sintaxe dos classificadores do `scikit-learn`\n",
    "- O método fit(X,Y) recebe uma matriz ou dataframe X onde cada linha é uma amostra de aprendizado, e um array Y contendo as saídas esperadas do classificador, seja na forma de texto ou de inteiros\n",
    "- O método predict(X) recebe uma matriz ou dataframe X onde cada linha é uma amostra de teste, retornando um array de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro médio de classificação:  0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "# 5. Executar o classificador nas amostras de validação\n",
    "classes_valid = mlp.predict(treinamento_pca[indices_valid,:])\n",
    "\n",
    "#Cálculo do erro\n",
    "erro = np.sum(classes_valid != alvos[indices_valid])/indices_valid.shape[0]\n",
    "print(\"Erro médio de classificação: \", erro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
